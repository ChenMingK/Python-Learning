## 参考资料
《深度学习之 PyTorch 实战计算机视觉》

## 监督学习和无监督学习
监督学习（Supervised Learning）和无监督学习（Unsupervised Learning）是在机器学习中经常被提及的两个重要的学习方法。

下面举一个例子来区分这两个概念：

假设有一堆苹果和梨混在一起组成水果，需要设计一个机器对这堆水果按苹果和梨分类，但是这个机器现在并不知道苹果和梨什么样，所以我们需要拿一些苹果和梨的照片告诉机器苹果和梨长什么样。经过多轮训练后，机器已经能够准确地对照片中的水果类别进行判断，并且对苹果和梨的特征形成自己的定义。之后我们的机器对这堆水果进行分类，这堆水果就能被准确地按类别分开。这就是监督学习的过程。

如果我们没有拿苹果和梨的照片对机器进行系统训练，机器也不知道苹果和梨长什么样，而是直接让机器对这一堆水果进行分类，则机器能够根据自己的 “直觉” 将这一堆水果准确地分成两类。这就是一个无监督学习的过程，说明机器自己总结出了苹果和梨的特征。

### 监督学习
我们可以对监督学习做如下简单定义：提供一组输入数据和其对应的标签数据，然后搭建一个模型，让模型通过训练后准确地找到输入数据和标签数据之间的最优映射关系，在输入新的数据后，模型能够通过之前学到的最优映射关系，快速地预测出这组数据的标签。

在实际应用中有两类问题使用监督学习的频次较高，其分别是回归问题和分类问题。

1、回归问题

回归问题就是使用监督学习的方法，让我们搭建的模型在通过训练后建立起一个连续的线性映射关系，其重点如下：
- 通过提供数据训练模型，让模型得到映射关系并能对新的输入数据进行预测。
- 得到的映射模型是线性连续的对应关系。


<img src="https://github.com/ChenMingK/ImagesStore/blob/master/python/1.png"></img>

线性回归的适用场景是我们已经获得一部分有对应关系的原始数据，并且问题的最终答案是得到一个连续的线性映射关系，其过程就是使用原始数据对建立的初始模型不断地进行训练，让模型不断拟合和修正，最终得到我们想要的线型模型，这个线性模型能够对我们之后输入的新数据准确地进行预测。



2、分类问题

分类问题就是让我们搭建的模型在通过监督学习之后建立起一个 **离散** 的映射关系。分类问题和回归问题在本质上有很大的不同，它依然需要使用提供的数据训练模型让模型得到映射关系，并能够对新的输入数据进行预测，不过最终得到的映射模型是一种离散的对应关系。

比如下图的肿瘤的属性和尺寸大小的关系：这里只将肿瘤的属性分为恶性肿瘤和良性肿瘤，0 表示良性，1 表示恶性。这个分类模型的输出模型只有两个，所以我们也通常把这种类型的分类模型叫做作二分类模型。

<img src="https://github.com/ChenMingK/ImagesStore/blob/master/python/2.png"></img>

分类模型的输出结果有时不仅仅有两个，也可以有多个，多分类问题与二分类问题相比会更复杂。

### 无监督学习
我们可以对无监督学习做如下简单定义：提供一组 **没有任何标签** 的输入数据，将其在我们搭建好的模型中进行训练，对整个训练过程不作任何干涉，最后得到一个能够发现数据之间隐藏特征的映射模型，使用这个映射模型能够实现对新数据的分类，这就是一个监督学习的过程。

无监督学习主要依靠模型自己寻找数据中隐藏的规律和特征，人工参与的成分远远少于监督学习的过程。

<img src="https://github.com/ChenMingK/ImagesStore/blob/master/python/3.png"></img>

如图所示，左图显示的是监督学习中的一个二分类模型，因为每个数据都有自己唯一对应的标签，这个标签在图中体现为叉号或者圆点；右图显示的就是无监督学习的过程，虽然数据也被最终分成了两类，但没有相应的数据标签，统一使用圆点表示。这就像实现了将具有相似关系的数据聚集在一起，所以使用无监督学习实现分类的算法又叫作 **聚类**。

### 小结
通过总结以上内容，我们发现监督学习和无监督学习的主要区别如下：
- 监督学习需要我们投入大量的精力处理原始数据，也因为我们的紧密参与使得最后得到的模型更符合设计者的需求和初衷。
- 无监督学习更具创造性，有时还能挖掘出数据之间我们意向不到的关系，不过最后的结果也可能会向不好的方向发展。

监督学习和无监督学习各有利弊，此外还有半监督学习和弱监督学习等更具创新的方法出现，只有理解各种学习方法的优缺点，才知道在实际场景中使用哪种学习方法能更好地解决问题。

## 欠拟合和过拟合
我们可以将搭建的模型是否发生欠拟合或者过拟合作为评价模型的拟合程度好坏的指标。拥有欠拟合特性的模型对已有数据的匹配性很差，不过对数据中的噪声不敏感；而拥有过拟合特性的模型对已有数据的匹配性太好，所以对数据中的噪声非常敏感。

### 欠拟合
在解决欠拟合问题时，主要从以下三方面着手：

(1) **增加特征项**：在大多数情况下出现欠拟合是因为我们没有准确地把握数据的主要特征，所以我们可以尝试在模型中加入更多的和原数据有重要相关性的特征来训练搭建的模型，这样得到的模型可能会有更好的泛化能力。

(2) **构造复杂的多项式**：我们知道一次项函数就是一条直线，二次项函数就是一条抛物线，一次项和二次项函数的特性决定了它们的泛化能力是有局限性的，如果数据不在直线或者抛物线的附近，那么必然出现欠拟合的情形，所以我们可以通过增加函数中的次项来增强模型的变化能力，从而提升其泛化能力。

(3) **减少正则化参数**：正则化参数出现的目的其实是防止过拟合情形的出现，但是如果我们的模型已经出现了欠拟合的情形，就可以通过减少正则化参数来消除欠拟合。

### 过拟合
过拟合模型受原数据中的噪声数据影响非常严重。

要解决在实践中遇到的过拟合问题，主要从以下三方面着手：

(1) **增大训练的数据量**：在大多数情况下发生过拟合是因为我们用于模型训练的数据量太小，搭建的模型过渡捕获了数据的有限特征，这是就会出现过拟合，在增加参与模型训练的数据量后，模型自然就能捕获数据的更多特征，就不会过于依赖数据的个别特征了。

(2) **采用正则化方法**：正则化一般指在目标函数之后加上范数，用来防止模型过拟合的发生，在实践中最常用到的正则化方法有 L0 正则、L1 正则和 L2 正则。

(3) **Dropout 方法**：Dropout 方法在神经网络模型中使用的频率较高，简单来说就是在神经网络模型进行前向传播的过程中，随机选取和丢弃指定层次之间的部分神经连接，因为整个过程是随机的，所以能有效防止过拟合的发生。

## 前向传播和后向传播
[https://blog.csdn.net/weixin\_38347387/article/details/82936585](https://blog.csdn.net/weixin_38347387/article/details/82936585)

## 损失和优化
深度神经网络中的损失用于度量。我们的模型得到的预测值和数据真实值之间的差距，也是一个用来衡量我们训练出来的模型泛化能力好坏的重要指标。模型预测值和真实值的差距越大，损失值就会越高。

对模型进行优化的最终目的是尽可能地在不过拟合的情况下降低损失值。在拥有一部分数据的真实值后，就可通过模型获得这部分数据的预测值，然后计算预测值与真实值之间的损失值，通过不断地优化模型参数来使这个损失值变得尽可能小。

### 损失函数
三种在深度学习中常用的损失函数分别是均方误差函数、均方根误差函数和平方绝对误差函数。

1、均方误差函数

均方误差（Mean Square Error，简称 MSE）函数计算的是预测值与真实值之差的平方的期望值，可用于评价数据的变化程度，其得到的值越小，则说明模型的预测值具有更好的精确度。其计算公式如下：

